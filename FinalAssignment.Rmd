---
title: "PML Course Project"
author: "Anil Puppala"
date: "May 1, 2016"
output: 
  html_document: 
    keep_md: yes
---


## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

In this project we will use the data generated by these devices to create a model/models to predict the manner in which the 6 subjects did the exercise, and apply the same learning (model) to predict 20 external cases as well.

The manner of exercise, which is the outcome in this analysis, is a factor variable with 5 levels. The five levels are: 'A', 'B', 'C', 'D', and 'E'.

## Loading Data
```{r , echo=TRUE}
library(rattle)
library(randomForest)
library(ggplot2)
library(caret)
library(forecast)
set.seed(77789)

sample <- read.csv("C:/R study material/Practical Machine Learning/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
assign20 <- read.csv("C:/R study material/Practical Machine Learning/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```

## Cross validation datasets - with training, testing and validation
I'm using three datasets. 1) Training, 2) Testing, and 3) Validation
```{r , echo=TRUE}
inBuild <- createDataPartition(y=sample$classe, p=0.7, list=FALSE)
validation <- sample[-inBuild,]; 
buildData <- sample[inBuild,]

inTrain <- createDataPartition(y=buildData$classe,  p=0.6, list=FALSE)
training <- buildData[inTrain,]; 
testing <- buildData[-inTrain,]
```

## Cleaning Data & Variable Reduction
The below code removes the features with near zero variability variables, variables with 65% 'NA's. Since the outcome is a categorical variable, I'm using non-parametric classification analysis. The analysis combines three models using caret platform. The three methods are: 1) CART Tree (rpart), 2) Gradient Boosted, and 3) Linear Discriminant Analysis

```{r , echo=TRUE}
training <- training[,-1]

varvar <- nearZeroVar(training, saveMetrics = T)
finvars <- varvar[,4]==FALSE
new_training <- training[,finvars]

finaltrain <- new_training 

for(gg in 1:length(new_training)) { 
  if( sum( is.na( new_training[, gg] ) ) /nrow(new_training) >= .5 ) { 
    for(kk in 1:length(finaltrain)) {
      if( length( grep(names(new_training[gg]), names(finaltrain)[kk]) ) ==1)  { 
        finaltrain <- finaltrain[ , -kk] 
      }   
    } 
  }
}
```

## Fitting Three Competing Models

```{r , echo=FALSE}
fit1 <- train(classe ~ ., data = finaltrain, method='rpart2')
fit2 <- train(classe ~ ., data = finaltrain, method='gbm', verbose=FALSE)
fit3 <- train(classe ~ ., data = finaltrain, method='lda2')
```

## Predicting the competing models with the out of sample (testing) set

```{r , echo=TRUE}
predFit1 <- predict(fit1, newdata = testing)
predFit2 <- predict(fit2, newdata = testing)
predFit3 <- predict(fit3, newdata = testing)
```

## Combining the three outcomes to fit a combined prediction

```{r , echo=TRUE}
predDF <- data.frame(predFit1, predFit2, predFit3, classe=testing$classe)
combFit <- train(classe ~ ., data = predDF)
predCombine <- predict(combFit, newdata = predDF)
```

## Compute accuracy of all three and the combined model

```{r , echo=TRUE}
testConfu1 <- confusionMatrix(predFit1, testing$classe)
testConfu2 <- confusionMatrix(predFit2, testing$classe)
testConfu3 <- confusionMatrix(predFit3, testing$classe)
testConfu4 <- confusionMatrix(predCombine, testing$classe)
```


## Predicting the out of sample (validation) sample
```{r , echo=TRUE}
validfit1 <- predict(fit1, newdata=validation)
validfit2 <- predict(fit3, newdata=validation)
validfit3 <- predict(fit3, newdata=validation)
```

## Prepare dataset for the combined prediction on the validation sample
```{r , echo=TRUE}
validDF <- data.frame(predFit1=validfit1, predFit2=validfit2, predFit3=validfit3)
predValid <- predict(combFit, validDF)
```

## Compare the accuracy for all the four models
```{r , echo=TRUE}
valitConfu1 <- confusionMatrix(validfit1, validation$classe)
valitConfu2 <- confusionMatrix(validfit2, validation$classe)
valitConfu3 <- confusionMatrix(validfit3, validation$classe)
valitConfu4 <- confusionMatrix(predValid, validation$classe)
valitConfu1
valitConfu2
valitConfu3
valitConfu4
```

## Predicting the 20 sample
The combined fit's accuracy is exactly as accurate as the rpart fit. I used the combined fit to predict the 20 sample
```{r , echo=TRUE}
as20Fit1 <- predict(fit1, newdata = assign20)
as20Fit2 <- predict(fit2, newdata = assign20)
as20Fit3 <- predict(fit3, newdata = assign20)

as20CombDat <- data.frame(predFit1=as20Fit1, predFit2=as20Fit2, predFit3=as20Fit3)

as20ComFit <- predict(combFit, newdata = as20CombDat)

as20ComFit
```

## Conclusion
I used the combined fit instead of rpart fit even though the accuracy is the same (refer to Appendix - Figure-1). I used it as the Error distribution shows lower variability (refer to Appendix - Figure-2).

##Appendix

Figure-1
```{r , echo=FALSE}
par(mfrow=c(2,2))


plot(valitConfu1$byClass[,8], xaxt="n", xlab="Classe", ylab="Accuracy", main="rpart Accuracy")
axis(1, at=1:5, labels = rownames(valitConfu1$byClass), las=2)
lines(valitConfu1$byClass[,8], type="l", col="blue")
abline(h=mean(valitConfu1$byClass[,8]), col="green")


plot(valitConfu2$byClass[,8], xaxt="n", xlab="Classe", ylab="Accuracy", main="GBM Accuracy")
axis(1, at=1:5, labels = rownames(valitConfu2$byClass), las=2)
lines(valitConfu2$byClass[,8], type="l", col="blue")
abline(h=mean(valitConfu2$byClass[,8]), col="green")

plot(valitConfu3$byClass[,8], xaxt="n", xlab="Classe", ylab="Accuracy", main="LDA Accuracy")
axis(1, at=1:5, labels = rownames(valitConfu3$byClass), las=2)
lines(valitConfu3$byClass[,8], type="l", col="blue")
abline(h=mean(valitConfu3$byClass[,8]), col="green")

plot(valitConfu4$byClass[,8], xaxt="n", xlab="Classe", ylab="Accuracy", main="Combined Fit Accuracy")
axis(1, at=1:5, labels = rownames(valitConfu4$byClass), las=2)
lines(valitConfu4$byClass[,8], type="l", col="blue")
abline(h=mean(valitConfu4$byClass[,8]), col="green")
```

Figure-2
```{r , echo=FALSE}
par(mfrow=c(1,1))
plot(combFit$finalModel, main="Combined Fit Errors Dist.")
```
